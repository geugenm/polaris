{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using XGBoost to infer links between telemetry\n",
    "\n",
    "XGBoost stands for eXtreme Gradient BOOSTing.\n",
    "It is an optimized parallel tree boosting method (GBDT Gradient Boosting Decision Tree). Similar to random forest, XGBoost does boosting; it is based on weak learners that have high bias (HB) and low variance (LV), boosting reduces inference errors by reducing the bias to get low bias (LB) and by the same occasion lower variance even more.\n",
    "Alike Random forest which uses fully grown decision tree with low bias and high variance, before ensembling to mainly reduce the variance.\n",
    "\n",
    "XGBoost -> Ensemble(HB, LV) = (LB, LV)\n",
    "Random Forest -> Ensemble(LB, HV) = (LB, LV)\n",
    "\n",
    "XGBoost model evolution is less random than Random Forest as it uses knowledge from not-fully-grown decision trees to build new ones. It is also faster as it does not grow trees completely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T22:43:01.606719Z",
     "start_time": "2018-11-13T22:43:01.599836Z"
    }
   },
   "source": [
    "## Starting with importation of xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T23:24:12.691639Z",
     "start_time": "2018-11-13T23:24:12.302453Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost parametrization is the tricky part. We could search what are the best parameters since we have data we can train on and test. But the following parameters will do fine for what we want; an estimation of dependencies, and not an exact prediction.\n",
    "\n",
    "Your prediction errors can be reduced by changing these parameters but, usually, the importance/impact order of predictors (telemetry parameters used to predict one of them) does not change significantly.\n",
    "\n",
    "More info on parameters here: https://xgboost.readthedocs.io/en/latest/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T23:24:13.394780Z",
     "start_time": "2018-11-13T23:24:13.380112Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.1,   # epsilon on slides\n",
    "    'gamma': 0,             # minimum loss reduction to make a split \n",
    "    'max_depth': 10,        # maximum depth of the tree\n",
    "    'n_estimators': 50,     # number of boosted trees to fit\n",
    "    \n",
    "    'base_score': 0.5,      # the initial prediction score of all instances, global bias.\n",
    "    'colsample_bylevel': 1, # subsample ratio of columns for each split, in each level\n",
    "    'colsample_bytree': 1,  # subsample ratio of columns when constructing each tree.\n",
    "    'max_delta_step': 0,    # maximum delta step we allow each tree's weight estimation to be    \n",
    "    'min_child_weight': 1,  # minimum sum of instance weight(hessian) needed in a child.\n",
    "    'missing': None,        # how to represent missing value, defaults to np.nan\n",
    "    'nthread': -1,          # number of parallel threads to run xgboost\n",
    "    'objective': \"reg:linear\", # or 'binary:logistic'  or \n",
    "    'reg_alpha': 0,         # L1 regularization term on weights\n",
    "    'reg_lambda': 1,        # L2 regularization term on weights\n",
    "    'scale_pos_weight': 1,  # balance of positive and negative weights\n",
    "    'seed': 0,              # randomness\n",
    "    'silent': True,         # verbosity\n",
    "    'subsample': 1,         # subsample ratio of the training instance.\n",
    "    \n",
    "    # I do not really have a GPU on my laptop so I'll wait warming my hands with the CPU :)\n",
    "    'predictor': \"cpu_predictor\", # or 'gpu_predictor'\n",
    "    'tree_method': \"auto\"         # or \"approx\" or \"(gpu_)hist\" or \"(gpu_)exact\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T23:17:20.328855Z",
     "start_time": "2018-11-13T23:17:20.322002Z"
    }
   },
   "source": [
    "## Creating model object to be trained\n",
    "\n",
    "    We use function *XGBClassifier* that returns a scikit-learn designed model.\n",
    "    This permits to use pandas dataframe with it and standard scikit-learn interfaces .fit() .predict() .score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T23:24:15.319965Z",
     "start_time": "2018-11-13T23:24:15.312787Z"
    }
   },
   "outputs": [],
   "source": [
    "# scikit-learn is needed for this form of model\n",
    "model = xgb.XGBClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
